{
 "cells": [
  {
   "cell_type": "raw",
   "id": "47100ca5-3ade-4f98-83bc-999a95f29a9b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Fine-tuning pretrained model for code translation\"\n",
    "author: \"Gian Cunningham\"\n",
    "date: \"2024-06-10\"\n",
    "categories: [ai, code]\n",
    "image: \"eye-catching-thumbnail.jpg\"\n",
    "---\n",
    "\n",
    "Discover how to fine-tune a pretrained sequence2sequence model to translate between 2 coding languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef34f32-4b04-46cf-85ee-d94966921503",
   "metadata": {},
   "source": [
    "# Setup and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350c23b-d2ba-4071-98bc-a0cd150d35cd",
   "metadata": {},
   "source": [
    "## Installing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f69248-7f5b-4203-9bdc-08ad50d836b5",
   "metadata": {},
   "source": [
    "We are installing the necessary libraries: **transformers** for our model, **datasets** for handling data, **torch** for PyTorch functionalities, and **antlr4-python3-runtime** for parsing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ecce3-474d-4295-9262-b39b04dcf7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate sacrebleu torch\n",
    "!pip install antlr4-python3-runtime==4.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49063d3-c5c3-4c98-bc3b-2f0e94e08a12",
   "metadata": {},
   "source": [
    "## Loading and prepping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6bd990-b259-4483-86ad-b6f791a95a5f",
   "metadata": {},
   "source": [
    "We are loading the dataset from JSON files using the datasets library. The dataset is split into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fca91317-10bf-417c-bba9-634056d3ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"data/train4.json\",\n",
    "    \"validation\": \"data/test2.json\"\n",
    "}\n",
    "data = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cbc07-b159-44a2-938c-7d12ec69be36",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894ce4a-f228-458c-b896-c17a1cb337bc",
   "metadata": {},
   "source": [
    "We are loading a pre-trained BART tokenizer from the Hugging Face hub, which we will use to preprocess our text data. We also define the source and target languages and a prefix for the translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daffac85-9439-406c-b34f-eb9b9a31e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, force_download=True)\n",
    "\n",
    "source_lang = \"pli\"\n",
    "target_lang = \"ktl\"\n",
    "prefix = \"translate PL/I to Kotlin: \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac116a-028c-4112-bbfd-484be2bc5256",
   "metadata": {},
   "source": [
    "We define a preprocessing function to prepare the data for the model. This function adds a prefix to the source text, tokenizes both inputs and targets, and truncates them to a maximum length. We will then apply this function to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f5c366-31e0-49b5-931a-eafd3c62c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + \" \".join(example) for example in examples[source_lang]]\n",
    "    targets = [\" \".join(example) for example in examples[target_lang]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954f2af-3192-4ec4-8faa-b10f62b0c983",
   "metadata": {},
   "source": [
    "## Create a data collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee7e23-284d-4cca-aadf-1e9e63455a44",
   "metadata": {},
   "source": [
    "We are creating a data collator that will dynamically pad the inputs and targets during batching to ensure they have the same length, which is necessary for efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d2a54c-9174-4e82-9787-dea8a07afeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73288d-b4b7-4bf7-b1b8-85b705488d02",
   "metadata": {},
   "source": [
    "## Initialize the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fc369-f48c-42ca-93de-c65eb9361b6a",
   "metadata": {},
   "source": [
    "We are loading the BART model, which is pre-trained for sequence-to-sequence tasks, from the Hugging Face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8f9c0f-a94b-4c71-a5c4-04ba493e0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8deb17-7edc-4842-839d-54a817d8079f",
   "metadata": {},
   "source": [
    "## Set Up Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00dc71f-5c89-4645-bdf1-a384880feb5a",
   "metadata": {},
   "source": [
    "We are setting up the training arguments, including the output directory, evaluation strategy, learning rate, batch sizes, weight decay, number of epochs, and logging strategy. These arguments configure the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd33cef-e618-4df6-9ceb-eeb87a3061af",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"pli_to_kotlin\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=150,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c21aa2-1628-4c38-847d-a3f28735cb20",
   "metadata": {},
   "source": [
    "## Initialize the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2ed84-7c2e-4668-8f7c-10a86358f75f",
   "metadata": {},
   "source": [
    "We are initializing the Seq2SeqTrainer with our model, training arguments, datasets, tokenizer, and data collator. The trainer handles the training and evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85cf5e5c-33cb-4a80-972b-ae7d7f6f52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b73a5-8f13-441e-94ec-d7e86a27e61a",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b877f-af6a-47f5-afc9-cfd55e1e0862",
   "metadata": {},
   "source": [
    "We are starting the training process using the trainer we initialized. This will train the model on our dataset for the specified number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a48d6c-cfe3-4190-ab79-2fdbd6e007c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:17, Epoch 150/150]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.627490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.627490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.001041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.610912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.070260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.903501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.693196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.557779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.404213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>1.291395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>1.219311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>1.145925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>1.066801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>0.965301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>0.875958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>0.773280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>0.671749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>0.563846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.689200</td>\n",
       "      <td>0.477218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.423841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.403889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.395815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.364311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.322163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.279419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.247632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.226813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.204244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.442100</td>\n",
       "      <td>0.181445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.146371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.127590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.122380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.138191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.125136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.108509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.095008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.076688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.064930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.062876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.069511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.081735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.091402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.095675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.098438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.091909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.077313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.062942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.054162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.047167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.042963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.040598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.036419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.033806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.034189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.036994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.039537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.042995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.047135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.049972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.052802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.052796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.045802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.033364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.025032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.018428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.014630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.012811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.012338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.013050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.014372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.016971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.019472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.020858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.021422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.020793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.020557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.018267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.015835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.014401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.013914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.013613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.012620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.012362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.011394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.011418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.010863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.010226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.009916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.009916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.009739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.009935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.009921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.010273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.011062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.011836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.012699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.013880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.016441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.016945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.016886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.015549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.014427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.012910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.011452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.010544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.009225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.008018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.007219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.006821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.006595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.006169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.005794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.005496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.005316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.005058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.004861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.004730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.004767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.004781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.004849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.005019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.004926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.004872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.004702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.004676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.004727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.004847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.005113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.005336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.005514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.005651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.005779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.005918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.006065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.006233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.006407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.006580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.006766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.006876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.006909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.007019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.007124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.007167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.007019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.006968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.006914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.006855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.006784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.006781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.006777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.5374304989973704, metrics={'train_runtime': 18.391, 'train_samples_per_second': 130.499, 'train_steps_per_second': 8.156, 'total_flos': 40013955072000.0, 'train_loss': 0.5374304989973704, 'epoch': 150.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697ddb7-55dc-437c-8675-658f8fa1be28",
   "metadata": {},
   "source": [
    "## Saving the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea862c-4cc1-4020-bdf4-7197fa3ae891",
   "metadata": {},
   "source": [
    "We are saving the trained model and tokenizer to the specified directory. This allows us to reuse the trained model for inference later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a82ce446-3c9f-4ebb-abc5-7e34d6a0720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./article_checkpoint/tokenizer_config.json',\n",
       " './article_checkpoint/special_tokens_map.json',\n",
       " './article_checkpoint/vocab.json',\n",
       " './article_checkpoint/merges.txt',\n",
       " './article_checkpoint/added_tokens.json',\n",
       " './article_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./article_checkpoint\")\n",
    "tokenizer.save_pretrained(\"./article_checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694095f2-089d-4af0-8619-db063ed9fc6e",
   "metadata": {},
   "source": [
    "## Running inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b756b-89d9-44e4-a1fa-67a6a20038bb",
   "metadata": {},
   "source": [
    "We are loading the saved model and tokenizer for inference. This allows us to actually use the trained model to translate new PL/I code to Kotlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a801a72-c3fb-4e6f-8208-04b4091a388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./article_checkpoint\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./article_checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e15d8-e245-4d0a-822c-59fb0e31dd69",
   "metadata": {},
   "source": [
    "We are setting up the device (GPU if available, otherwise CPU) and moving the model to the appropriate device for efficient computation during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42b30cef-95d9-4f71-a899-64df97ad5423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e3da20-09b7-4f08-9343-c47b78888ed9",
   "metadata": {},
   "source": [
    "The function tokenizes the input, generates the translated sequence, and decodes it back to a readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9010e2f-a38b-41f1-a09b-e8fa4d13c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sequence(sentence, tokenizer, model, device, max_length=50):\n",
    "    inputs = tokenizer(prefix + sentence, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    translated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6b1f1-5519-48e5-9bd8-c5e431931232",
   "metadata": {},
   "source": [
    "We are defining a function to format the translated Kotlin code by handling indentation and using Jinja2 templates to insert context-specific data into the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7adc2b7f-a23f-45d5-87da-6c2e01aefc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "def transpile_sequence(translated, context, level=0):\n",
    "    tokens = translated.split()\n",
    "    lint = []\n",
    "    current_line = \"\"\n",
    "    for t in tokens:\n",
    "        t = context.get(t, t)\n",
    "        if t in [\"{\", \"}\"]:\n",
    "            if current_line:\n",
    "                lint.append(\"\".rjust(level * 4) + current_line.strip())\n",
    "                current_line = \"\"\n",
    "            if t == \"{\":\n",
    "                lint.append(\"\".rjust(level * 4) + t)\n",
    "                level += 1\n",
    "            elif t == \"}\":\n",
    "                level -= 1\n",
    "                lint.append(\"\".rjust(level * 4) + t)\n",
    "        else:\n",
    "            current_line += \" \" + t\n",
    "    if current_line:\n",
    "        lint.append(\"\".rjust(level * 4) + current_line.strip())\n",
    "\n",
    "    formatted_code = \"\\n\".join(lint)\n",
    "    template = Template(formatted_code)\n",
    "    rendered_code = template.render(context)\n",
    "    return rendered_code, level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f418c-7f6f-4b46-8843-d05f7a27ea18",
   "metadata": {},
   "source": [
    "We are defining a function to parse a PL/I file, translate each statement to Kotlin, and transpile the translated code. The function uses ANTLR to parse the PL/I code and a visitor pattern to extract statements for translation and transpilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efb3f724-b357-4534-adb0-cbc9f995872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from antlr4 import *\n",
    "from pli.PLILexer import PLILexer\n",
    "from pli.PLIParser import PLIParser\n",
    "from pli.PLIVisitor import PLIVisitor\n",
    "from jinja2 import Template\n",
    "\n",
    "def parse_and_translate(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        original_code = file.read()\n",
    "    print(\"PL/I Code:\")\n",
    "    print(original_code)\n",
    "    print()\n",
    "\n",
    "    # Lexer and parser setup\n",
    "    input_stream = InputStream(original_code)\n",
    "    lexer = PLILexer(input_stream)\n",
    "    stream = CommonTokenStream(lexer)\n",
    "    parser = PLIParser(stream)\n",
    "    tree = parser.program()\n",
    "\n",
    "    # Visitor setup\n",
    "    visitor = PLIVisitor()\n",
    "    statements = visitor.visit(tree)\n",
    "\n",
    "    # Translate and transpile each statement\n",
    "    transpiled_code = \"\"\n",
    "    level = 0\n",
    "    for stmt in statements:\n",
    "        pli_code = \" \".join(stmt['pli'])\n",
    "        context = stmt.get('context', {})\n",
    "        translated = translate_sequence(pli_code, tokenizer, model, device)\n",
    "        transpiled, level = transpile_sequence(translated, context, level)\n",
    "        transpiled_code += transpiled + \"\\n\"\n",
    "\n",
    "    print(\"Kotlin Code:\")\n",
    "    print(transpiled_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a431b86-d930-4934-8804-be30730805c2",
   "metadata": {},
   "source": [
    "We are specifying the filename of the PL/I file to be translated and transpiled and then calling the parse_and_translate function to process the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cca59ad3-a74f-4047-bbf4-b87424226a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PL/I Code:\n",
      " Factorial: proc options (main);\n",
      "    dcl (n,result) fixed bin(31);\n",
      "    n  = 5;\n",
      "    result = Compute_factorial(n);\n",
      "\n",
      " end Factorial;\n",
      "  /***********************************************/\n",
      "  /* Subroutine                                  */\n",
      "  /***********************************************/\n",
      "  Compute_factorial: proc (n)  returns (fixed bin(31));\n",
      "     dcl n fixed bin(15);\n",
      "      if n <= 1 then\n",
      "        return(1);\n",
      "\n",
      "     return( n*Compute_factorial(n-1) );\n",
      "\n",
      "  end Compute_factorial;\n",
      "\n",
      "\n",
      "Kotlin Code:\n",
      "fun main (args: Array<String>)\n",
      "{\n",
      "    var n : Int\n",
      "    var result : Int\n",
      "    n = 5\n",
      "    result = compute_factorial(n)\n",
      "}\n",
      "fun compute_factorial(n : Int) : Int\n",
      "{\n",
      "    var n : Int\n",
      "    if(n<=1)\n",
      "    {\n",
      "        return 1\n",
      "    }\n",
      "    return n*compute_factorial(n-1)\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "filename = \"FIB.PLI\"  # Replace with the actual filename\n",
    "parse_and_translate(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
