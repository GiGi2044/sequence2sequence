{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb7a759-0017-4bdb-8459-3838fbfd0af0",
   "metadata": {},
   "source": [
    "## Setup & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045cb64-9e6c-4fc7-8fcf-322316724ab8",
   "metadata": {},
   "source": [
    "We first need to install all the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917d3a92-8216-440c-9725-4229020d464e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T12:47:56.946535Z",
     "iopub.status.busy": "2024-05-06T12:47:56.945391Z",
     "iopub.status.idle": "2024-05-06T12:48:06.765130Z",
     "shell.execute_reply": "2024-05-06T12:48:06.762911Z",
     "shell.execute_reply.started": "2024-05-06T12:47:56.946535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting antlr4-python3-runtime==4.9.2\n",
      "  Downloading antlr4-python3-runtime-4.9.2.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.2-py3-none-any.whl size=144547 sha256=9b50d8998d848e902ef007d5368032df056e2b14e166a97ecc0482624d4a501e\n",
      "  Stored in directory: /root/.cache/pip/wheels/1d/2f/50/8609b0155597d16b276307cb7899e7c832412f4d0d3a1db01c\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime\n",
      "Successfully installed antlr4-python3-runtime-4.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install antlr4-python3-runtime==4.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c274e5a-731d-4b95-9d24-95224bbb8231",
   "metadata": {},
   "source": [
    "We will then import all the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51eada17-310b-48bf-8d60-41517057a9d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T12:48:06.769778Z",
     "iopub.status.busy": "2024-05-06T12:48:06.769121Z",
     "iopub.status.idle": "2024-05-06T12:48:08.619964Z",
     "shell.execute_reply": "2024-05-06T12:48:08.618457Z",
     "shell.execute_reply.started": "2024-05-06T12:48:06.769723Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset,DatasetDict, load_dataset\n",
    "import pandas as pd\n",
    "from antlr4 import *\n",
    "from pli.PLILexer import PLILexer\n",
    "from pli.PLIParser import PLIParser\n",
    "from pli.PLIVisitor import PLIVisitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad59674-4efb-44da-9ab4-c96ed14c6d1b",
   "metadata": {},
   "source": [
    "We reformatted the data set now so that we can use a standard load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f206ed-f235-449c-83f0-459836e32738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T12:48:08.623373Z",
     "iopub.status.busy": "2024-05-06T12:48:08.622603Z",
     "iopub.status.idle": "2024-05-06T12:48:09.123751Z",
     "shell.execute_reply": "2024-05-06T12:48:09.121733Z",
     "shell.execute_reply.started": "2024-05-06T12:48:08.623310Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2cdb10d3c3013ad5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-2cdb10d3c3013ad5/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed8e3952c2e4646a76a424699831da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40daf11f8fd4d3f85cfb1f7d80ed367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2c951792da46489a065727694d28eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-2cdb10d3c3013ad5/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70708ffb9724ce08c2315b4232038ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"/notebooks/data/train1.json\", field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158fbfe3-fc10-4909-abf9-f8b5604aef73",
   "metadata": {},
   "source": [
    "lets just display the dataset to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adac31ed-b930-4795-b602-8ca590714df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T12:48:12.667533Z",
     "iopub.status.busy": "2024-05-06T12:48:12.666896Z",
     "iopub.status.idle": "2024-05-06T12:48:12.686104Z",
     "shell.execute_reply": "2024-05-06T12:48:12.683048Z",
     "shell.execute_reply.started": "2024-05-06T12:48:12.667486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee92cc2-0b96-4f63-a18c-353ce055215d",
   "metadata": {},
   "source": [
    "Great, lest split it up, and create validation and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2dae960-c217-469d-9d3f-8d25b88fa5f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T12:48:15.980198Z",
     "iopub.status.busy": "2024-05-06T12:48:15.979615Z",
     "iopub.status.idle": "2024-05-06T12:48:16.010098Z",
     "shell.execute_reply": "2024-05-06T12:48:16.008038Z",
     "shell.execute_reply.started": "2024-05-06T12:48:15.980153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 9\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets = data[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e399a79b-c39b-45ba-805c-0db5b00eeabd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T12:48:17.724559Z",
     "iopub.status.busy": "2024-05-06T12:48:17.724559Z",
     "iopub.status.idle": "2024-05-06T12:48:17.736611Z",
     "shell.execute_reply": "2024-05-06T12:48:17.734772Z",
     "shell.execute_reply.started": "2024-05-06T12:48:17.724559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ktl': 'fun main (args: {{type0}} <{{type1}}>)',\n",
       " 'pli': 'PROCEDURE MAIN {{type0}} {{type1}}'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7567b-b023-431a-afda-9368c52479e3",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787993a7-e00c-42a0-9244-62098beb70ff",
   "metadata": {},
   "source": [
    "Import a tokenizer to convert all our inputs and targets\n",
    "\n",
    "TOKENs in compiler and LLM/Transformers have different meanings. Tokenisation here refer to the creation of a numerical value for the input tokens, a vector that can be used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4508a59f-01c1-43b0-a94a-cd782de35856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T12:55:26.084777Z",
     "iopub.status.busy": "2024-05-06T12:55:26.083542Z",
     "iopub.status.idle": "2024-05-06T12:55:27.490895Z",
     "shell.execute_reply": "2024-05-06T12:55:27.488580Z",
     "shell.execute_reply.started": "2024-05-06T12:55:26.084712Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google-t5/t5-base\" # Replace this with your desired model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35668556-8d22-481f-9c26-c17263c158c9",
   "metadata": {},
   "source": [
    "Note because we merged the dataset, we can actually use the same dataset for both source and target.\n",
    "The most interesting thing here is the tokenizer, a hugging face function, that produces the attention_mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b22ff9d5-9e08-4360-978d-1304bb2c8e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T12:55:31.681836Z",
     "iopub.status.busy": "2024-05-06T12:55:31.681228Z",
     "iopub.status.idle": "2024-05-06T12:55:31.702432Z",
     "shell.execute_reply": "2024-05-06T12:55:31.700469Z",
     "shell.execute_reply.started": "2024-05-06T12:55:31.681823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[ 6828,   254,  2326, 18290,   283, 13570,     3,     2,  6137,   632,\n",
       "              2,     3,     2,  6137,   536,     2,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[ 694,  711,   41, 8240,    7,   10,    3,    2, 6137,  632,    2,    3,\n",
       "             2, 6137,  536,    2, 3155,   61,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pli_sentence = split_datasets[\"train\"][1][\"translation\"][\"pli\"]\n",
    "ktl_sentence = split_datasets[\"train\"][1][\"translation\"][\"ktl\"]\n",
    "\n",
    "inputs = tokenizer(pli_sentence, return_tensors=\"pt\")\n",
    "targets = tokenizer(ktl_sentence, return_tensors=\"pt\")\n",
    "inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "414c0ed3-5aa5-4575-83fa-b8311882a964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁PRO', 'C', 'ED', 'URE', '▁M', 'AIN', '▁', '<unk>', 'type', '0', '<unk>', '▁', '<unk>', 'type', '1', '<unk>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(pli_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c8139c-4094-4cbc-9dca-81893f0f7aef",
   "metadata": {},
   "source": [
    "A quick function to clean up the input sequence and set the model up to accept the input sequence\n",
    "\n",
    "overflowing_tokens and num_truncated tokens are things like whitespace, and start of sequence/end of sequence etc.\n",
    "\n",
    "This model expects the inputs to be named \"labels\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9745afd2-1614-4663-83b7-47bd4aa82b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:15:24.939669Z",
     "iopub.status.busy": "2024-05-04T23:15:24.939117Z",
     "iopub.status.idle": "2024-05-04T23:15:24.951826Z",
     "shell.execute_reply": "2024-05-04T23:15:24.949832Z",
     "shell.execute_reply.started": "2024-05-04T23:15:24.939626Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"pli\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"ktl\"] for ex in examples[\"translation\"]]\n",
    "    \n",
    "    # Tokenize inputs and targets separately\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=True)\n",
    "    model_targets = tokenizer(targets, max_length=max_length, truncation=True, padding=True)\n",
    "    \n",
    "    # Remove unnecessary keys from model_inputs\n",
    "    model_inputs.pop(\"overflowing_tokens\", None)\n",
    "    model_inputs.pop(\"num_truncated_tokens\", None)\n",
    "    \n",
    "    # Add targets to model_inputs\n",
    "    model_inputs[\"labels\"] = model_targets[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c5cfbb-a1c4-4aef-b15e-1009a9a118d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:16:40.431843Z",
     "iopub.status.busy": "2024-05-04T23:16:40.430179Z",
     "iopub.status.idle": "2024-05-04T23:16:41.506482Z",
     "shell.execute_reply": "2024-05-04T23:16:41.502829Z",
     "shell.execute_reply.started": "2024-05-04T23:16:40.431843Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2cdb10d3c3013ad5/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-cde49ae52028470c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2cdb10d3c3013ad5/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-238adf16c7a61394.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285baca-5cab-4e3f-9391-694e565deb5f",
   "metadata": {},
   "source": [
    "## Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba07cf00-c66f-4f55-be7d-cea8d6260f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:16:55.621564Z",
     "iopub.status.busy": "2024-05-04T23:16:55.620916Z",
     "iopub.status.idle": "2024-05-04T23:16:58.972916Z",
     "shell.execute_reply": "2024-05-04T23:16:58.970836Z",
     "shell.execute_reply.started": "2024-05-04T23:16:55.621511Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4496ec7d-aaee-4613-875b-74edeed9b420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:17:10.165029Z",
     "iopub.status.busy": "2024-05-04T23:17:10.164443Z",
     "iopub.status.idle": "2024-05-04T23:17:10.188212Z",
     "shell.execute_reply": "2024-05-04T23:17:10.187112Z",
     "shell.execute_reply.started": "2024-05-04T23:17:10.164984Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bbf4f43-8a7f-4ad7-974b-584527cafc3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:17:17.353976Z",
     "iopub.status.busy": "2024-05-04T23:17:17.352329Z",
     "iopub.status.idle": "2024-05-04T23:17:17.379649Z",
     "shell.execute_reply": "2024-05-04T23:17:17.377768Z",
     "shell.execute_reply.started": "2024-05-04T23:17:17.353961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b51d62c5-3e9c-41a1-9eb1-c5dd603d626e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:17:18.991413Z",
     "iopub.status.busy": "2024-05-04T23:17:18.990338Z",
     "iopub.status.idle": "2024-05-04T23:17:19.073920Z",
     "shell.execute_reply": "2024-05-04T23:17:19.069745Z",
     "shell.execute_reply.started": "2024-05-04T23:17:18.991413Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c82cf52-56be-4ab7-9f04-2ae452904de8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:17:19.971030Z",
     "iopub.status.busy": "2024-05-04T23:17:19.969955Z",
     "iopub.status.idle": "2024-05-04T23:17:20.020494Z",
     "shell.execute_reply": "2024-05-04T23:17:20.018264Z",
     "shell.execute_reply.started": "2024-05-04T23:17:19.971030Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",  # No evaluation during training\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=150,\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b03f6d98-ec8c-4d3d-8787-b227aa42cae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:17:21.260492Z",
     "iopub.status.busy": "2024-05-04T23:17:21.259918Z",
     "iopub.status.idle": "2024-05-04T23:17:28.435972Z",
     "shell.execute_reply": "2024-05-04T23:17:28.433681Z",
     "shell.execute_reply.started": "2024-05-04T23:17:21.260450Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=None,  # Assuming no evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc5b384b-b40b-4e2a-b979-647d0c68c2d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:17:31.291555Z",
     "iopub.status.busy": "2024-05-04T23:17:31.289555Z",
     "iopub.status.idle": "2024-05-04T23:17:31.301881Z",
     "shell.execute_reply": "2024-05-04T23:17:31.299297Z",
     "shell.execute_reply.started": "2024-05-04T23:17:31.291478Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d136e02f-d8a2-4177-9699-dfcd2f4f8ff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T23:17:33.489795Z",
     "iopub.status.busy": "2024-05-04T23:17:33.488223Z",
     "iopub.status.idle": "2024-05-04T23:18:01.326171Z",
     "shell.execute_reply": "2024-05-04T23:18:01.324188Z",
     "shell.execute_reply.started": "2024-05-04T23:17:33.489738Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9\n",
      "  Num Epochs = 150\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 150\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:22, Epoch 150/150]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.14063475290934244, metrics={'train_runtime': 27.7835, 'train_samples_per_second': 48.59, 'train_steps_per_second': 5.399, 'total_flos': 8938045440000.0, 'train_loss': 0.14063475290934244, 'epoch': 150.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
